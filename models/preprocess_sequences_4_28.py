# preprocess_sequences_optimized.py
import pandas as pd
import numpy as np
import os
import gc
from tqdm import tqdm
import networkx as nx
from collections import defaultdict, Counter
import pickle
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import cupy as cp

    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False


def generate_user_sequences_optimized(behavior_path, output_path, chunk_size=2_000_000):
    """ä¼˜åŒ–ç‰ˆç”¨æˆ·è¡Œä¸ºåºåˆ—ç”Ÿæˆï¼ˆå•æ¬¡éå†+å†…å­˜ä¼˜åŒ–ï¼‰"""
    print(f"ğŸš€ å¼€å§‹å¤„ç†ç”¨æˆ·è¡Œä¸ºåºåˆ—: {behavior_path}")

    # é…ç½®æ•°æ®ç±»å‹
    dtype = {
        'user': 'int32',
        'cate': 'int16',
        'brand': 'int16',
        'time_stamp': 'int32'
    }

    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    user_sequences = defaultdict(list)

    # å¸¦è¿›åº¦æ¡çš„å—å¤„ç†
    with tqdm(desc="å¤„ç†æ•°æ®å—", unit='chunk') as pbar:
        for chunk in pd.read_csv(behavior_path, chunksize=chunk_size, dtype=dtype):
            # é¢„è®¡ç®—ç»„åˆé”®å¹¶æ’åº
            chunk['item_pair'] = chunk['cate'].astype(str) + ',' + chunk['brand'].astype(str)
            sorted_chunk = chunk.sort_values(['user', 'time_stamp'])

            # åˆ†ç»„èšåˆ
            grouped = sorted_chunk.groupby('user')['item_pair'].agg('|'.join)

            # åˆå¹¶ç»“æœ
            for user, seq in grouped.items():
                user_sequences[user].append(seq)

            # å†…å­˜æ¸…ç†
            del chunk, sorted_chunk, grouped
            gc.collect()
            pbar.update(1)

    # å†™å…¥æœ€ç»ˆç»“æœ
    print("ğŸ› ï¸ åˆå¹¶ç”¨æˆ·åºåˆ—...")
    with open(output_path, 'w') as f_out:
        f_out.write("user,hist_sequence\n")
        for user, parts in tqdm(user_sequences.items(), desc="å†™å…¥æ–‡ä»¶"):
            full_seq = '|'.join(parts)
            f_out.write(f"{user},{full_seq}\n")

    print(f"âœ… ç”¨æˆ·åºåˆ—ä¿å­˜è‡³ {output_path} (å…± {len(user_sequences)} ç”¨æˆ·)")


def build_cooccurrence_graph(behavior_path, chunk_size=1_000_000):
    """é«˜æ•ˆæ„å»ºå…±ç°å›¾"""
    print("ğŸ•¸ï¸ æ„å»ºå…±ç°å…³ç³»å›¾...")
    co_occur = defaultdict(Counter)

    # è®¡ç®—æ€»å—æ•°
    with open(behavior_path, 'r') as f:
        total_chunks = sum(1 for _ in f) // chunk_size + 1

    # åˆ†å—å¤„ç†
    with tqdm(total=total_chunks, desc="å¤„ç†æ•°æ®å—") as pbar:
        for chunk in pd.read_csv(
                behavior_path,
                chunksize=chunk_size,
                dtype={'user': 'int32', 'cate': 'int16', 'brand': 'int16'}
        ):
            # ç”Ÿæˆç‰©å“æ ‡è¯†
            chunk['item'] = chunk['cate'].astype(str) + '_' + chunk['brand'].astype(str)

            # å¤„ç†ç”¨æˆ·åºåˆ—
            for _, group in chunk.groupby('user'):
                items = group.sort_values('time_stamp')['item'].values
                for i in range(len(items) - 1):
                    co_occur[items[i]].update([items[i + 1]])

            pbar.update(1)
            del chunk
            gc.collect()

    return co_occur


def generate_item_sequences_optimized(behavior_path, output_path, walks_per_node=5):
    """å¹¶è¡Œä¼˜åŒ–ç‰ˆç‰©å“åºåˆ—ç”Ÿæˆ"""
    # 1. æ„å»ºå…±ç°å›¾
    co_occur = build_cooccurrence_graph(behavior_path)

    # 2. æ„å»ºå›¾ç»“æ„
    print("ğŸ—ï¸ åˆ›å»ºå›¾ç½‘ç»œ...")
    G = nx.DiGraph()
    for src, counters in tqdm(co_occur.items(), desc="æ·»åŠ è¾¹"):
        total = sum(counters.values())
        for dst, count in counters.items():
            G.add_edge(src, dst, weight=count / total)

    # 3. é¢„è®¡ç®—è½¬ç§»æ¦‚ç‡
    print("âš¡ é¢„è®¡ç®—è½¬ç§»çŸ©é˜µ...")
    transition_probs = {}
    nodes = list(G.nodes())
    for node in tqdm(nodes, desc="å¤„ç†èŠ‚ç‚¹"):
        neighbors = list(G.successors(node))
        if neighbors:
            probs = [G[node][n]['weight'] for n in neighbors]
            transition_probs[node] = (neighbors, np.array(probs))

    # 4. å¹¶è¡Œç”Ÿæˆæ¸¸èµ°åºåˆ—
    print(f"ğŸš¶ ç”Ÿæˆéšæœºæ¸¸èµ° (å¹¶è¡Œ workers={os.cpu_count()})...")
    with open(output_path, 'w') as f_out:
        f_out.write("item,graph_seq\n")

        def generate_walks(node):
            walks = []
            for _ in range(walks_per_node):
                walk = [node]
                current = node
                for _ in range(10):
                    if current not in transition_probs:
                        break
                    neighbors, probs = transition_probs[current]
                    next_node = np.random.choice(neighbors, p=probs)
                    walk.append(next_node)
                    current = next_node
                return f"{node},{' '.join(walk)}\n"

        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = [executor.submit(generate_walks, node) for node in nodes]
            for future in tqdm(as_completed(futures), total=len(nodes), desc="ç”Ÿæˆåºåˆ—"):
                f_out.write(future.result())

    return G


def save_graph(graph, output_path):
    """ä¿å­˜å›¾å¯¹è±¡"""
    with open(output_path, 'wb') as f:
        pickle.dump(graph, f)
    print(f"âœ… å›¾ç»“æ„å·²ä¿å­˜è‡³ {output_path}")


def memory_guard(func):
    """å†…å­˜ä¿æŠ¤è£…é¥°å™¨"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except MemoryError:
            print("\nâš ï¸ å†…å­˜ä¸è¶³ï¼å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            print("1. å‡å° chunk_size å‚æ•°")
            print("2. ä½¿ç”¨æ›´é«˜å†…å­˜çš„æœºå™¨")
            print("3. å¢åŠ äº¤æ¢ç©ºé—´ï¼ˆswap spaceï¼‰")
            print("4. ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹ï¼ˆå¦‚int8ï¼‰")
            exit(1)

    return wrapper


@memory_guard
def main():
    # é…ç½®è·¯å¾„
    os.makedirs("../data", exist_ok=True)
    behavior_path = "../data/cleaned_behavior.csv"
    user_seq_path = "../data/sequence/user_sequences_optimized.csv"
    item_seq_path = "../data/sequence/item_sequences_optimized.csv"
    graph_path = "../data/item_graph_optimized.pkl"

    # æ‰§è¡Œå¤„ç†æµç¨‹
    start_time = time.time()

    # ç”¨æˆ·åºåˆ—ç”Ÿæˆ
    generate_user_sequences_optimized(behavior_path, user_seq_path)

    # ç‰©å“åºåˆ—ç”Ÿæˆ
    item_graph = generate_item_sequences_optimized(behavior_path, item_seq_path)
    save_graph(item_graph, graph_path)

    # æ€§èƒ½æŠ¥å‘Š
    total_time = time.time() - start_time
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æ€»ç”¨æ—¶: {total_time // 3600:.0f}h {total_time % 3600 // 60:.0f}m {total_time % 60:.2f}s")

    # å†…å­˜æ¸…ç†
    gc.collect()


if __name__ == "__main__":
    main()